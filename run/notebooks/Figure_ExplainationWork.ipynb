{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import os\n",
    "os.chdir('project/learning_causal_discovery/')\n",
    "from run.shallowmind.api.infer import prepare_inference\n",
    "from pytorch_grad_cam import GradCAM\n",
    "\n",
    "\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "root_dir = 'work_dir/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# helper function\n",
    "def normalization(x, eps=1e-6):\n",
    "        x = abs(x)\n",
    "        # rescale operations to ensure gradients lie between 0 and 1\n",
    "        flatin = x.reshape((x.size(0),-1))\n",
    "        temp, _ = flatin.min(1, keepdim=True)\n",
    "        x = x - temp.unsqueeze(1)\n",
    "\n",
    "        flatin = x.reshape((x.size(0),-1))\n",
    "        temp, _ = flatin.max(1, keepdim=True)\n",
    "        x = x / (temp.unsqueeze(1) + eps)\n",
    "        return x\n",
    "\n",
    "def get_input_gradient(model, objective, x):\n",
    "    model.zero_grad()\n",
    "    # gradients w.r.t. input\n",
    "    input_gradients = torch.autograd.grad(outputs=objective, inputs=x)[0]\n",
    "    return input_gradients\n",
    "\n",
    "def get_gradient_saliency(model, sample):\n",
    "    x, y = copy.deepcopy(sample)\n",
    "    x = {k: torch.tensor(v).unsqueeze(0).cuda() for k, v in x.items()}\n",
    "    x['seq'].requires_grad_()\n",
    "    model.eval()\n",
    "    prob = model(x)\n",
    "    objective = -1. * F.nll_loss(prob, y.cuda().flatten(), reduction='sum')\n",
    "    input_gradient = get_input_gradient(model, objective, x['seq'])\n",
    "    # Input-gradient * image\n",
    "    # input_gradient = input_gradient * x['seq']\n",
    "    input_gradient = rearrange(input_gradient, 'b l c -> b c l')\n",
    "    gradient = normalization(input_gradient).cpu()\n",
    "    gradient = rearrange(gradient, 'b c l -> b l c')\n",
    "    return {'seq':x['seq'], 'confidence': prob.softmax(dim=-1)[0, 1], 'gt': y, 'gradient': gradient.squeeze().reshape(-1, 2)}\n",
    "\n",
    "class GradCAM1d(GradCAM):\n",
    "    def get_cam_weights(self, input_tensor, target_layer, target_category, activations, grads):\n",
    "        return np.mean(grads, axis=1)\n",
    "\n",
    "    def scale_cam_image(self, cam, target_size=None):\n",
    "        result = []\n",
    "        for img in cam:\n",
    "            img = img - np.min(img)\n",
    "            img = img / (1e-7 + np.max(img))\n",
    "            if target_size is not None:\n",
    "                img = np.interp(np.linspace(0, target_size, target_size), np.linspace(0, target_size, len(img)), img)\n",
    "            result.append(img)\n",
    "        result = np.float32(result)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_target_length(self, input_tensor):\n",
    "        length = input_tensor.size(-2)\n",
    "        return length\n",
    "\n",
    "    def get_cam_image(self,\n",
    "                      input_tensor,\n",
    "                      target_layer,\n",
    "                      targets,\n",
    "                      activations,\n",
    "                      grads,\n",
    "                      eigen_smooth=False):\n",
    "\n",
    "        weights = self.get_cam_weights(input_tensor,\n",
    "                                       target_layer,\n",
    "                                       targets,\n",
    "                                       activations,\n",
    "                                       grads)\n",
    "        weighted_activations = weights[:, :, None] * np.transpose(activations, (0, 2, 1))\n",
    "        cam = np.transpose(weighted_activations, (0, 2, 1)).sum(axis=-1)\n",
    "        return cam\n",
    "\n",
    "    def compute_cam_per_layer(\n",
    "            self,\n",
    "            input_tensor,\n",
    "            targets,\n",
    "            eigen_smooth):\n",
    "        activations_list = [a.cpu().data.numpy()\n",
    "                            for a in self.activations_and_grads.activations]\n",
    "        grads_list = [g.cpu().data.numpy()\n",
    "                      for g in self.activations_and_grads.gradients]\n",
    "        target_size = self.get_target_length(input_tensor)\n",
    "\n",
    "        cam_per_target_layer = []\n",
    "        # Loop over the saliency image from every layer\n",
    "        for i in range(len(self.target_layers)):\n",
    "            target_layer = self.target_layers[i]\n",
    "            layer_activations = None\n",
    "            layer_grads = None\n",
    "            if i < len(activations_list):\n",
    "                layer_activations = activations_list[i]\n",
    "            if i < len(grads_list):\n",
    "                layer_grads = grads_list[i]\n",
    "\n",
    "            cam = self.get_cam_image(input_tensor,\n",
    "                                     target_layer,\n",
    "                                     targets,\n",
    "                                     layer_activations,\n",
    "                                     layer_grads,\n",
    "                                     eigen_smooth)\n",
    "            cam = np.maximum(cam, 0)\n",
    "            scaled = self.scale_cam_image(cam, target_size)\n",
    "            cam_per_target_layer.append(scaled[:, None, :])\n",
    "\n",
    "        return cam_per_target_layer\n",
    "\n",
    "def get_cam_plots(target_idxs, mi, dl, axes=None, shift=False, default_shift='backward', save_dir=None):\n",
    "    target_layers = [mi.model.backbone.trm.layers[3].norm1]\n",
    "    cam = GradCAM1d(model=mi, target_layers=target_layers, use_cuda=True)\n",
    "\n",
    "    # targets = [ClassifierOutputTarget(1)]\n",
    "\n",
    "    # grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "    # print(grayscale_cam.shape)\n",
    "    if axes is None:\n",
    "        fig, axes = plt.subplots(round(len(target_idxs)/3), 3, figsize=(24, round(len(target_idxs)/3)*4))\n",
    "\n",
    "    cls = ['#DE1334', '#6752FF', '#0B2735']\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        if shift:\n",
    "            if isinstance(dl.dataset[idx][0]['seq'], np.ndarray):\n",
    "                input_tensor = torch.tensor(dl.dataset[i][0]['seq']).unsqueeze(0).cuda()\n",
    "            else:\n",
    "                input_tensor = dl.dataset[target_idxs[i]][0]['seq'].clone().unsqueeze(0).cuda()\n",
    "            if default_shift == 'forward':\n",
    "                input_tensor[:, :, 1] = torch.cat((input_tensor[0, 0, 1].reshape(1, 1, 1).tile(1, 200, 1), input_tensor[:, :-200, 1].unsqueeze(-1)), dim=1).squeeze()\n",
    "            elif default_shift == 'backward':\n",
    "                input_tensor[:, :, 1] = torch.cat((input_tensor[:, 200:, 1].unsqueeze(-1), input_tensor[0, -1, 1].reshape(1, 1, 1).tile(1, 200, 1)), dim=1).squeeze()\n",
    "            else:\n",
    "                raise ValueError('default_shift must be either forward or backward')\n",
    "        else:\n",
    "            input_tensor =  torch.tensor(dl.dataset[target_idxs[i]][0]['seq']).unsqueeze(0).cuda()\n",
    "        grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "        ax.plot(dl.dataset[target_idxs[i]][0]['seq'][::100, 0], c=cls[0], linestyle='-', label='cause')\n",
    "        ax.plot(dl.dataset[target_idxs[i]][0]['seq'][::100, 1], c=cls[1], linestyle='--', label='result')\n",
    "        # if i < len(target_idxs)//2:\n",
    "        #     ax.plot(dl.dataset[target_idxs[i]][0]['seq'][:, 1], c=cls[1], label='result')\n",
    "        # else:\n",
    "        #     ax.plot(dl.dataset[target_idxs[i]][0]['seq'][:, 1], c=cls[2], label='result')\n",
    "        ax.plot(grayscale_cam.squeeze()[::100], c=cls[2], label='grad_cam')\n",
    "        confidence = mi(input_tensor).softmax(dim=-1)[:, 1].detach().cpu().numpy()[0]\n",
    "        label = int(target_idxs[i] in pos_samples)\n",
    "        if shift:\n",
    "            ax.set_title(f'Original Label: {label} | Current Label: {int(label==0)} | Prediction: {int(confidence>0.5)}', fontdict={'family': 'Serif'})\n",
    "        else:\n",
    "            ax.set_title(f'Label: {label} | Prediction: {int(confidence>0.5)}', fontdict={'family': 'Serif'})\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.legend(loc='lower right', prop={'family': 'Serif'}, labelcolor='black')\n",
    "\n",
    "    if save_dir is not None:\n",
    "        plt.savefig(f\"{save_dir}.svg\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Gradient Saliency"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get result from deep learning method\n",
    "cfgs = [\n",
    "        'patch_transformer_128_50ep_cosine_adamw_1e-3lr_256bs_0.05wd_fold_seed_42/patch_transformer_128_50ep_cosine_adamw_1e-3lr_256bs_0.05wd.py'\n",
    "        ]\n",
    "ckpts = [\n",
    "         'patch_transformer_128_50ep_cosine_adamw_1e-3lr_256bs_0.05wd_fold_seed_42/ckpts/exp_name=patch_transformer_128_50ep_cosine_adamw_1e-3lr_256bs_0.05wd-cfg=patch_transformer_128_50ep_cosine_adamw_1e-3lr_256bs_0.05wd-bs=256-val_auroc=0.9380.ckpt'\n",
    "         ]\n",
    "cfgs, ckpts = [root_dir + cfg for cfg in cfgs], [root_dir + ckpt for ckpt in ckpts]\n",
    "\n",
    "di, mi = prepare_inference(cfgs[0], ckpts[0])\n",
    "mi = mi.cuda()\n",
    "di.setup()\n",
    "dl = di.test_dataloader()\n",
    "\n",
    "cfgs = [\n",
    "        'patch_transformer_128_0.5noise_fold_seed_42/patch_transformer_128_w0.5noise.py'\n",
    "        ]\n",
    "ckpts = [\n",
    "         'patch_transformer_128_0.5noise_fold_seed_42/ckpts/exp_name=patch_transformer_128_0.5noise-cfg=patch_transformer_128_w0.5noise-bs=256-val_auroc=0.9493.ckpt'\n",
    "         ]\n",
    "cfgs, ckpts = [root_dir + cfg for cfg in cfgs], [root_dir + ckpt for ckpt in ckpts]\n",
    "\n",
    "di_noise, mi_noise = prepare_inference(cfgs[0], ckpts[0])\n",
    "mi_noise = mi_noise.cuda()\n",
    "di_noise.setup()\n",
    "dl_noise = di_noise.test_dataloader()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# get postive and negative samples\n",
    "df = pd.read_csv(di.hparams.data.test.split)\n",
    "pos_samples = np.where(df['label']==1)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GradCAM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, 3, figsize=(24, 24))\n",
    "target_idxs = pos_samples[0:45:5].tolist()\n",
    "get_cam_plots(target_idxs, mi, dl, axes.flatten()[:9])\n",
    "get_cam_plots(target_idxs, mi_noise, dl_noise, axes.flatten()[9:18])\n",
    "fig.savefig('/home/charon/project/nmos_inference/figures/Figure 5. CAM/cam_supplement.svg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_cam_plots(target_idxs, mi, dl, shift=True, save_dir='/home/charon/project/nmos_inference/figures/Figure 5. CAM/reverse')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-sensorium-py",
   "language": "python",
   "display_name": "Python [conda env:sensorium]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}