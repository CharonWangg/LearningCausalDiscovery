{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T02:26:16.127584Z",
     "iopub.status.busy": "2022-10-13T02:26:16.127002Z",
     "iopub.status.idle": "2022-10-13T02:26:16.312147Z",
     "shell.execute_reply": "2022-10-13T02:26:16.311483Z",
     "shell.execute_reply.started": "2022-10-13T02:26:16.127561Z"
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.functional import auroc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "plt.style.use('bmh')\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "os.chdir('project/learning_causal_discovery/')\n",
    "\n",
    "from run.shallowmind.api.infer import prepare_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-13T02:26:16.705625Z",
     "iopub.status.busy": "2022-10-13T02:26:16.704821Z",
     "iopub.status.idle": "2022-10-13T02:26:16.708666Z",
     "shell.execute_reply": "2022-10-13T02:26:16.708092Z",
     "shell.execute_reply.started": "2022-10-13T02:26:16.705599Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# root directory of model checkpoint and config files\n",
    "work_dir = 'work_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-13T02:26:16.917043Z",
     "iopub.status.busy": "2022-10-13T02:26:16.916463Z",
     "iopub.status.idle": "2022-10-13T02:26:16.923868Z",
     "shell.execute_reply": "2022-10-13T02:26:16.923140Z",
     "shell.execute_reply.started": "2022-10-13T02:26:16.917021Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def search_cfg_ckpt(target_dir, keyword=None, screen=None, target_suffix=[\"ckpt\", \"py\"]):\n",
    "    # search files by given keywords and suffix, and screened keywords under target directory\n",
    "    find_res = []\n",
    "    target_suffix_dot = [\".\" + suffix for suffix in target_suffix]\n",
    "    walk_generator = os.walk(target_dir)\n",
    "    for root_path, dirs, files in walk_generator:\n",
    "        if len(files) < 1:\n",
    "            continue\n",
    "        for file in files:\n",
    "            file_name, suffix_name = os.path.splitext(file)\n",
    "            if suffix_name in target_suffix_dot:\n",
    "                file_name = os.path.join(root_path, file)\n",
    "                # keyword check\n",
    "                if keyword is not None:\n",
    "                    _check = 0\n",
    "                    for word in keyword:\n",
    "                        if word in file_name:\n",
    "                            _check += 1\n",
    "                    if screen is not None:\n",
    "                        for screen_word in screen:\n",
    "                                if screen_word in file_name:\n",
    "                                    _check -= 1\n",
    "                    if _check == len(keyword):\n",
    "                            find_res.append(file_name)\n",
    "                else:\n",
    "                    find_res.append(file_name)\n",
    "    return find_res\n",
    "\n",
    "def cfg_ckpt2dict(files):\n",
    "    # convert config list and ckpt list in to dict {cfg: ckpt}\n",
    "    cfgs = [cfg for cfg in files if '.py' in cfg]\n",
    "    ckpts = [ckpt for ckpt in files if '.ckpt' in ckpt]\n",
    "    dict = {}\n",
    "    for cfg in cfgs:\n",
    "        root = cfg.split('/')[-2]\n",
    "        for i, ckpt in enumerate(ckpts):\n",
    "            if ckpt.split('/')[-3] == root:\n",
    "                dict[cfg] = ckpt\n",
    "                break\n",
    "        try:\n",
    "            del ckpts[i]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-10T05:00:03.769135Z",
     "iopub.status.busy": "2022-10-10T05:00:03.768566Z",
     "iopub.status.idle": "2022-10-10T05:00:04.191962Z",
     "shell.execute_reply": "2022-10-10T05:00:04.190998Z",
     "shell.execute_reply.started": "2022-10-10T05:00:03.769114Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "# plotting\n",
    "games = ['DonkeyKong', 'Pitfall', 'SpaceInvaders']\n",
    "methods = [\"corr_score\", \"mi_score\", \"gc_score\", \"lingam_score\",\"LSTM_score\", \"TCN_score\", \"Transformer_score\"]\n",
    "label = [\"Pearson Correlation\", \"Mutual Information\", \"Linear Granger Causality\", \"ICA-LiNGAM\", \"LSTM\", \"TCN\", \"Transformer\"]\n",
    "cls = [218, 240, 178, 146, 212, 185, 62, 179, 195, 91, 191, 192 ,30, 128, 184, 36, 65, 154, 10, 31, 93]\n",
    "\n",
    "def digits2color(digits):\n",
    "    d_group = [digits[i:i+3] for i in range(0, len(digits), 3)]\n",
    "    colors = [tuple((np.array(d)/255).tolist()) for d in d_group]\n",
    "    return colors\n",
    "\n",
    "cls = digits2color(cls)\n",
    "cls = cls*4\n",
    "\n",
    "def plot_regular_results(res, conditions, error_bar=False, plot_kwargs={}, save_dir=None):\n",
    "\n",
    "    if error_bar:\n",
    "        seeded_res = [[res[c][g][j][seed] for seed in res[c][g][j]] for c in conditions for g in res[c] for j in methods]\n",
    "        # (mean, std)\n",
    "        seeded_res = [(np.mean(seeds), np.std(seeds)) for seeds in seeded_res]\n",
    "    fig, ax1 = plt.subplots(figsize=(9, 7), constrained_layout=True, dpi=300)\n",
    "\n",
    "    title = plot_kwargs.get('title', f'Methods Comparison on DonkeyKong\\n')\n",
    "    ax1.set_title(title, fontdict={'family': 'Serif', 'size': 14})\n",
    "    ax1.set_xlabel(\n",
    "        'AUROC', fontdict={'family': 'Serif', 'size': 14})\n",
    "\n",
    "    # bars combo\n",
    "    width = plot_kwargs.get('width', 0.35)\n",
    "    interval = plot_kwargs.get('interval', 4)\n",
    "    total = len(conditions) * interval\n",
    "    y_labels = plot_kwargs.get('y_labels', [])\n",
    "    bar_height = plot_kwargs.get('bar_height', 0.35)\n",
    "\n",
    "    # go through the scores of different level of augmentation\n",
    "    x = [np.linspace(i-3*width, i+3*width, len(methods)) for i in range(0,total,interval)]\n",
    "    x = np.concatenate(x).reshape(-1)\n",
    "\n",
    "    if error_bar:\n",
    "        y = [res[c][g][j] for c in conditions for g in res[c] for j in methods]\n",
    "    else:\n",
    "        y = [value for c in conditions for g in res[c] for j in methods for seed, value in res[c][g][j].items()]\n",
    "\n",
    "    for i in range(len(methods)):\n",
    "        if error_bar:\n",
    "            mean, std = [s[0] for s in seeded_res], [s[1] for s in seeded_res]\n",
    "            p = ax1.barh(x[i::len(methods)], mean[i::len(methods)], xerr=std[i::len(methods)],\n",
    "                     color=cls[i], align='center', height=bar_height, label=label[i],\n",
    "                     error_kw={'linewidth': 2, 'capsize': 6})\n",
    "            ax1.bar_label(p, fmt='%.3f', fontfamily='Serif', fontsize=14)\n",
    "        else:\n",
    "            ax1.barh(x[i::len(methods)], y[i::len(methods)], color=cls[i], align='center', height=bar_height, label=label[i])\n",
    "\n",
    "    ax1.set_xlim([0.4, 1])\n",
    "    ax1.set_xticks(np.linspace(0.4, 0.9, 6))\n",
    "    ax1.xaxis.grid(True, linestyle='--', which='major',\n",
    "                   color='grey', alpha=.25)\n",
    "    ax1.axvline(0.5, color='#6e9ece', alpha=0.25)  # median position\n",
    "    ax1.set_yticks(x) if not y_labels else ax1.set_yticks(range(0, total, interval))\n",
    "    ax1.set_yticklabels(labels=y_labels, fontdict={'family': 'Serif', 'size': 14})\n",
    "    ax1.legend(loc='lower right', prop={'family': 'Serif', 'size': 14},)\n",
    "    if save_dir is not None:\n",
    "        plt.savefig(f\"{save_dir}.svg\")\n",
    "\n",
    "def collect_inferences(file_cfg=dict(), target_dir='.cache/sim_data/mos_auc_result'):\n",
    "    # get result from traditional method\n",
    "    # file name format: game={game}-method={method}-seed={seed}-note={note}.pkl\n",
    "    game = file_cfg.get('game', ['DonkeyKong'])\n",
    "    method = file_cfg.get('method', ['corr', 'mi', 'gc', 'lingam'])\n",
    "    seed = file_cfg.get('seed', [42])\n",
    "    note = file_cfg.get('note', ['default'])\n",
    "    result = {}\n",
    "\n",
    "    for _note in note:\n",
    "        result[_note] = {}\n",
    "        for _game in game:\n",
    "            result[_note][_game] = {}\n",
    "            for _method in method:\n",
    "                result[_note][_game][f'{_method}_score'] = {}\n",
    "                for _seed in seed:\n",
    "                    file_name = os.path.join(target_dir, f'game={_game}-method={_method}-seed={_seed}-note={_note}.pkl')\n",
    "                    file = pickle.load(open(file_name, 'rb'))\n",
    "                    if _method in ['mi', 'lingam']:\n",
    "                        # directly calculate AUC (coeff, adjacency matrices)\n",
    "                        result[_note][_game][f'{_method}_score'][_seed] = roc_auc_score(np.array(file[_method]['label']), np.array(file[_method]['pred']))\n",
    "                    elif _method in ['corr']:\n",
    "                        # calculate on the absolute value\n",
    "                        result[_note][_game][f'{_method}_score'][_seed] = roc_auc_score(np.array(file[_method]['label']), np.abs(np.array(file[_method]['pred'])))\n",
    "                    elif _method in ['gc']:\n",
    "                        # non-zero entries are regarded as causality\n",
    "                        result[_note][_game][f'{_method}_score'][_seed] = roc_auc_score(np.array(file[_method]['label']), np.where(np.array(file[_method]['pred'])!=0, 1, 0))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def models_inference(keyword, result, target_dir=root_dir, screen=None, cfg_hook=None):\n",
    "    mi_dicts = cfg_ckpt2dict(search_cfg_ckpt(target_dir, screen=screen, keyword=keyword))\n",
    "    print(f'total models number: {len(mi_dicts)}')\n",
    "    for idx, (cfg, ckpt) in tqdm(enumerate(mi_dicts.items())):\n",
    "        seed = int(cfg.split('/')[-2].split('_')[-1])\n",
    "        di, mi = prepare_inference(cfg, ckpt)\n",
    "        mi = mi.cuda()\n",
    "        if cfg_hook is not None:\n",
    "            di.data_cfg = cfg_hook(di.data_cfg)\n",
    "        di.setup()\n",
    "        di.data_cfg.test_batch_size = 1024\n",
    "        dl = di.test_dataloader()\n",
    "        preds = []\n",
    "        labels = []\n",
    "        mi.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in tqdm(enumerate(dl), total=len(dl), desc=f'{mi.hparams.model.backbone.type}'):\n",
    "                pred = mi({k: v.cuda() for k, v in x.items()})\n",
    "                preds.append(pred.softmax(dim=-1)[:, 1].detach().cpu())\n",
    "                labels.append(y.detach().cpu())\n",
    "        preds = torch.cat(preds, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "        if result.get(f'{mi.hparams.model.backbone.type}_score', None) is None:\n",
    "            result[f'{mi.hparams.model.backbone.type}_score'] = {}\n",
    "        result[f'{mi.hparams.model.backbone.type}_score'][seed] = auroc(preds, labels, pos_label=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Regular Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "condition = 'default'\n",
    "game = 'DonkeyKong'\n",
    "# collect inference result from traditional methods\n",
    "result = collect_inferences(file_cfg={'game': [game],\n",
    "                                        'method': ['corr', 'mi', 'gc', 'lingam'],\n",
    "                                        'seed': [42, 1207, 12, 7, 3057],\n",
    "                                        'note': [condition]}, target_dir='.cache/sim_data/mos_auc_result')\n",
    "# get result from deep learning method\n",
    "result[condition][game] = models_inference(keyword=[\"fold_seed\"], result=result[condition][game],\n",
    "                          target_dir=work_dir, screen=['noise'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_regular_results(result, error_bar=True, conditions=['default'], save_dir='figures/Figure 4. Methods/regular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate the mean and std of result\n",
    "for condition in result.keys():\n",
    "    for game in result[condition].keys():\n",
    "        for method in result[condition][game].keys():\n",
    "            result[condition][game][method] = np.array(list(result[condition][game][method].values()))\n",
    "            print(f'{condition} {game} {method} mean: {np.mean(result[condition][game][method])}, std: {np.std(result[condition][game][method])}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Noise Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conditions = ['0.1noise', '0.3noise', '0.5noise']\n",
    "game = 'DonkeyKong'\n",
    "# collect inference result from traditional methods\n",
    "result.update(collect_inferences(file_cfg={'game': [game],\n",
    "                                        'method': ['corr', 'mi', 'gc', 'lingam'],\n",
    "                                        'seed': [42, 1207, 12, 7, 3057],\n",
    "                                        'note': conditions}, target_dir='.cache/sim_data/mos_auc_result'))\n",
    "# get result from deep learning method\n",
    "for condition in ['0.1noise', '0.3noise', '0.5noise']:\n",
    "    result[condition][game] = models_inference(keyword=[condition], result=result[condition][game],\n",
    "                                                target_dir=work_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_regular_results(result, error_bar=True, conditions=['0.1noise'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_regular_results(result, error_bar=True, conditions=['0.3noise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plot_regular_results(result, error_bar=True, conditions=['0.5noise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "y_labels = ['Regular', 'Noise Scale 0.1', 'Noise Scale 0.3', 'Noise Scale 0.5']\n",
    "plot_regular_results(result, conditions=['default', '0.1noise', '0.3noise', '0.5noise'],\n",
    "                     error_bar=True, plot_kwargs={'y_labels': y_labels, 'bar_height':0.5, 'interval': 3},\n",
    "                     save_dir='figures/Figure 4. Methods/noise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cross Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# prediction by different seeds\n",
    "cross_result = {}\n",
    "condition = 'default'\n",
    "\n",
    "# collect inference result from traditional methods\n",
    "cross_result = collect_inferences(file_cfg={'game': ['DonkeyKong', 'Pitfall', 'SpaceInvaders'],\n",
    "                                        'method': ['corr', 'mi', 'gc', 'lingam'],\n",
    "                                        'seed': [42],\n",
    "                                        'note': [condition]}, target_dir='.cache/sim_data/mos_auc_result')\n",
    "# get result from deep learning method\n",
    "for game in ['DonkeyKong', 'Pitfall', 'SpaceInvaders']:\n",
    "    def testset_modifier(cfg):\n",
    "        cfg.test.data_root = f'.cache/sim_data/{game}/HR/' + 'Regular_3510_step_256_rec_2e3.npy'\n",
    "        cfg.test.split = f'.cache/sim_data/{game}/csv/fold_seed_42/' + 'test_sim_grouped.csv'\n",
    "        return cfg\n",
    "    cross_result[condition][game] = models_inference(['fold_seed_42'], result=cross_result[condition][game], screen=['noise'], target_dir=work_dir, cfg_hook=testset_modifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "y_labels = ['DonkeyKong', 'Pitfall', 'SpaceInvaders']\n",
    "# wrap the dict format to match the plot function\n",
    "_cross_result = {k: {k: cross_result['default'][k]} for k, v in cross_result['default'].items()}\n",
    "plot_regular_results(_cross_result, conditions=y_labels,\n",
    "                     plot_kwargs={'y_labels': y_labels, 'bar_height':0.5, 'interval': 3,\n",
    "                                  'title': 'Methods Comparison cross Games'},\n",
    "                     save_dir='figures/Figure 4. Methods/game')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NetSim"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checkpoint Inference"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T04:06:27.347300Z",
     "iopub.status.busy": "2022-10-13T04:06:27.346652Z",
     "iopub.status.idle": "2022-10-13T04:06:27.353476Z",
     "shell.execute_reply": "2022-10-13T04:06:27.353025Z",
     "shell.execute_reply.started": "2022-10-13T04:06:27.347277Z"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model inference\n",
    "def eval_netsim(cfg, ckpt, sim='default', add_noise=False):\n",
    "    di, mi = prepare_inference(cfg, ckpt)\n",
    "    mi = mi.cuda()\n",
    "    if sim != 'default':\n",
    "        di.data_cfg.test.data_root = sim\n",
    "        di.data_cfg.test.percentage = [0.6, 1.0]\n",
    "        if add_noise:\n",
    "            di.data_cfg.test.pipeline = [dict(type='TsAug', transforms=[dict(type='AddNoise', scale=0.5, normalize=True, seed=42)]),\n",
    "                                         dict(type='ToTensor')]\n",
    "    di.setup(stage='test')\n",
    "    di.data_cfg.test_batch_size = 1024*10\n",
    "    di.data_cfg.test_batch_size = 1024*10\n",
    "    dl = di.test_dataloader()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    mi.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in tqdm(enumerate(dl), total=len(dl), desc=f'{mi.hparams.model.backbone.type}'):\n",
    "            pred = mi({k: v.cuda() for k, v in x.items()})\n",
    "            preds.append(pred.softmax(dim=-1)[:, 1].detach().cpu())\n",
    "            labels.append(y.detach().cpu())\n",
    "\n",
    "        preds = torch.cat(preds, dim=0)\n",
    "        labels = torch.cat(labels, dim=0)\n",
    "    return preds, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T04:06:35.677429Z",
     "iopub.status.busy": "2022-10-13T04:06:35.676798Z",
     "iopub.status.idle": "2022-10-13T04:07:05.279705Z",
     "shell.execute_reply": "2022-10-13T04:07:05.278866Z",
     "shell.execute_reply.started": "2022-10-13T04:06:35.677408Z"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# directory of netsim dataset\n",
    "netsim_dir = '.cache/netsim'\n",
    "sims = [f'{netsim_dir}/sim{i}.mat' for i in [1, 2, 3, 4, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24]]\n",
    "\n",
    "mi_dicts = cfg_ckpt2dict(search_cfg_ckpt(keyword=['netsim', 'partial', 'transformer'], screen=['lag', 'norm'], target_dir=root_dir))\n",
    "\n",
    "cfgs, ckpts = [], []\n",
    "for k, v in mi_dicts.items():\n",
    "    cfgs.append(k)\n",
    "    ckpts.append(v)\n",
    "cfg, ckpt = cfgs[0], ckpts[0]\n",
    "\n",
    "print(f'total models number: {len(mi_dicts)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load precomputed result of other methods in NetSim and calculate AUC\n",
    "since doing inference by traditional methods is time consuming, we precomputed the result and save it in `.cache/sim_data/default_result.pkl` and `.cache/sim_data/noise_result.pkl` as we mentioned in the appendix.\n",
    "result = {'mi': [{'pred': pred, 'label': label}...], 'gc': [{'pred': pred, 'label': label}...]}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T04:02:59.413726Z",
     "iopub.status.busy": "2022-10-13T04:02:59.412935Z",
     "iopub.status.idle": "2022-10-13T04:02:59.598550Z",
     "shell.execute_reply": "2022-10-13T04:02:59.597996Z",
     "shell.execute_reply.started": "2022-10-13T04:02:59.413704Z"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "lcd_auc = []\n",
    "for sim in sims:\n",
    "    preds, labels = eval_netsim(cfg, ckpt, sim=sim)\n",
    "    # auc\n",
    "    lcd_auc.append(auroc(preds, labels))\n",
    "\n",
    "graph_auc = {}\n",
    "default_result = pickle.load(open('.cache/sim_data/netsim_auc_result/default_result.pkl', 'rb'))\n",
    "pred_sldisco, pred_ges, pred_gc, pred_mi, pred_lingam = default_result['sldisco'], default_result['ges'], default_result['gc'],  default_result['mi'], default_result['lingam']\n",
    "\n",
    "netsim_dir = '.cache/netsim'\n",
    "sims = [f'{netsim_dir}/sim{i}.mat' for i in [1, 2, 3, 4, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24]]\n",
    "\n",
    "# iterate over all the methods\n",
    "for method, pred in zip(['sldisco', 'ges', 'gc', 'mi', 'lingam'], [pred_sldisco, pred_ges, pred_gc, pred_mi, pred_lingam]):\n",
    "    auc = []\n",
    "    for sim, res in zip(sims, pred):\n",
    "        # auc\n",
    "        if not pred[sim]:\n",
    "            # for ges gc\n",
    "            assert method in ['ges', 'gc'], 'only ges and gc have no result'\n",
    "            auc.append(np.nan)\n",
    "        else:\n",
    "            dd = [(np.array(d['pred']).reshape(-1), np.where(np.array(d['label'])!=0, 1, 0).reshape(-1)) for d in pred[sim]]\n",
    "            auc.append(auroc(torch.from_numpy(np.concatenate([d[0] for d in dd])).to(torch.float32), torch.from_numpy(np.concatenate([d[1] for d in dd])).to(torch.int32)))\n",
    "    graph_auc[method] = auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T04:03:01.717619Z",
     "iopub.status.busy": "2022-10-13T04:03:01.717355Z",
     "iopub.status.idle": "2022-10-13T04:03:02.659833Z",
     "shell.execute_reply": "2022-10-13T04:03:02.659322Z",
     "shell.execute_reply.started": "2022-10-13T04:03:01.717601Z"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# default\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sldisco_auc, ges_auc, gc_auc, mi_auc, lingam_auc = graph_auc['sldisco'], graph_auc['ges'], \\\n",
    "                                                   graph_auc['gc'], graph_auc['mi'], graph_auc['lingam']\n",
    "\n",
    "cls = [218, 240, 178, 146, 212, 185, 62, 179, 195, 30, 128, 184, 36, 65, 154, 10,31, 93]\n",
    "\n",
    "def digits2color(digits):\n",
    "    d_group = [digits[i:i+3] for i in range(0, len(digits), 3)]\n",
    "    colors = [tuple((np.array(d)/255).tolist()) for d in d_group]\n",
    "    return colors\n",
    "cls = digits2color(cls)\n",
    "\n",
    "plt.figure(figsize=(10, 5), dpi=300)\n",
    "plt.plot(lcd_auc, linestyle='--', marker='o', color=cls[-1], label='Transformer')\n",
    "plt.plot(lingam_auc, linestyle='--', marker='s', color=cls[5], label='ICA-LiNGAM')\n",
    "plt.plot(mi_auc, linestyle='--', marker='^', color=cls[4], label='Mutual Information')\n",
    "plt.plot(sldisco_auc, linestyle='--', marker='<', color=cls[3], label='SLDisco')\n",
    "plt.plot(ges_auc, linestyle='--', marker='>', color=cls[2], label='GES')\n",
    "plt.plot(gc_auc, linestyle='--', marker='v', color=cls[1], label='Linear Granger Causality')\n",
    "plt.xticks(range(len(sims)), [sim.split('/')[-1].strip('.mat') for sim in sims ], rotation=90)\n",
    "plt.ylim(0.2, 1.0)\n",
    "plt.ylabel('AUROC')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig('figures/Figure 6. NetSim/default_result.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# noise\n",
    "lcd_auc = []\n",
    "for sim in sims:\n",
    "    preds, labels = eval_netsim(cfg, ckpt, sim=sim, add_noise=True)\n",
    "    # auc\n",
    "    lcd_auc.append(auroc(preds, labels))\n",
    "\n",
    "graph_auc = {}\n",
    "noise_result = pickle.load(open('.cache/sim_data/netsim_auc_result/noise_result.pkl', 'rb'))\n",
    "pred_sldisco, pred_ges, pred_gc, pred_mi, pred_lingam = noise_result['sldisco'], noise_result['ges'], noise_result['gc'], noise_result['mi'], noise_result['lingam']\n",
    "\n",
    "# iterate over all the methods\n",
    "for method, pred in zip(['sldisco', 'ges', 'gc', 'mi', 'lingam'], [pred_sldisco, pred_ges, pred_gc, pred_mi, pred_lingam]):\n",
    "    auc = []\n",
    "    for sim, res in zip(sims, pred):\n",
    "        # auc\n",
    "        if not pred[sim]:\n",
    "            # for ges gc\n",
    "            assert method in ['ges', 'gc'], 'only ges and gc have no result'\n",
    "            auc.append(np.nan)\n",
    "        else:\n",
    "            dd = [(np.array(d['pred']).reshape(-1), np.where(np.array(d['label'])!=0, 1, 0).reshape(-1)) for d in pred[sim]]\n",
    "            auc.append(auroc(torch.from_numpy(np.concatenate([d[0] for d in dd])).to(torch.float32), torch.from_numpy(np.concatenate([d[1] for d in dd])).to(torch.int32)))\n",
    "    graph_auc[method] = auc\n",
    "\n",
    "sldisco_auc, ges_auc, gc_auc, mi_auc, lingam_auc = graph_auc['sldisco'], graph_auc['ges'], \\\n",
    "                                                   graph_auc['gc'], graph_auc['mi'], graph_auc['lingam']\n",
    "\n",
    "cls = [218, 240, 178, 146, 212, 185, 62, 179, 195, 30, 128, 184, 36, 65, 154, 10,31, 93]\n",
    "\n",
    "def digits2color(digits):\n",
    "    d_group = [digits[i:i+3] for i in range(0, len(digits), 3)]\n",
    "    colors = [tuple((np.array(d)/255).tolist()) for d in d_group]\n",
    "    return colors\n",
    "cls = digits2color(cls)\n",
    "\n",
    "plt.figure(figsize=(10, 5), dpi=300)\n",
    "plt.plot(lcd_auc, linestyle='--', marker='o', color=cls[-1], label='Transformer')\n",
    "plt.plot(lingam_auc, linestyle='--', marker='s', color=cls[5], label='ICA-LiNGAM')\n",
    "plt.plot(mi_auc, linestyle='--', marker='^', color=cls[4], label='Mutual Information')\n",
    "plt.plot(sldisco_auc, linestyle='--', marker='<', color=cls[3], label='SLDisco')\n",
    "plt.plot(ges_auc, linestyle='--', marker='>', color=cls[2], label='GES')\n",
    "plt.plot(gc_auc, linestyle='--', marker='v', color=cls[1], label='Linear Granger Causality')\n",
    "plt.xticks(range(len(sims)), [sim.split('/')[-1].strip('.mat') for sim in sims], rotation=90)\n",
    "plt.ylim(0.2, 1.0)\n",
    "plt.ylabel('AUROC')\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig('figures/Figure 6. NetSim/noise_result.svg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "conda-env-dev_test-py",
   "language": "python",
   "display_name": "Python [conda env:dev_test]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}